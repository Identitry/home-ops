---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd/helm-controller/main/config/crd/bases/helm.toolkit.fluxcd.io_helmrepositories.yaml
# OCI Repository for n8n
apiVersion: source.toolkit.fluxcd.io/v1
kind: OCIRepository
metadata:
  name: n8n
  namespace: cozy-fluxcd
spec:
  interval: 5m
  layerSelector:
    mediaType: application/vnd.cncf.helm.chart.content.v1.tar+gzip
    operation: copy
  ref:
    tag: 2.0.1
  url: oci://8gears.container-registry.com/library/n8n
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/external-secrets/external-secrets/refs/heads/main/config/crds/bases/external-secrets.io_externalsecrets.yaml
# n8n application secrets (encryption key, etc.)
apiVersion: external-secrets.io/v1
kind: ExternalSecret
metadata:
  name: es-n8n-config
  namespace: ai-automation
spec:
  secretStoreRef:
    kind: ClusterSecretStore
    name: infisical
  target:
    name: n8n-config
    creationPolicy: Owner
    template:
      metadata:
        labels:
          reloader.stakater.com/auto: "true"
  data:
    - secretKey: encryption_key
      remoteRef:
        key: /AI_AUTOMATION/N8N_ENCRYPTION_KEY
    - secretKey: license_key
      remoteRef:
        key: /AI_AUTOMATION/N8N_LICENSE_KEY
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/spotahome/redis-operator/refs/heads/master/manifests/databases.spotahome.com_redisfailovers.yaml
# RedisFailover for n8n, used for caching and queue management
apiVersion: databases.spotahome.com/v1
kind: RedisFailover
metadata:
  name: n8n-redis
  namespace: ai-automation
spec:
  sentinel:
    replicas: 3
  redis:
    replicas: 3
---
# PersistentVolumeClaim for n8n data
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: n8n-data
  namespace: ai-automation
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 5Gi
  storageClassName: local
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd/helm-controller/main/config/crd/bases/helm.toolkit.fluxcd.io_helmreleases.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: n8n
  namespace: ai-automation
spec:
  chartRef:
    kind: OCIRepository
    name: n8n
    namespace: cozy-fluxcd
  interval: 1h
  driftDetection:
    mode: enabled
  values:
    replicaCount: 1

    deploymentStrategy:
      type: Recreate

    deploymentAnnotations:
      reloader.stakater.com/auto: "true"

    image:
      repository: n8nio/n8n
      pullPolicy: IfNotPresent
      tag: 2.6.2@sha256:ff6fe9eca746b7455c9e6e4fbc6f5753c3204e82279e015fa62f1bfe309e6343

    main:
      # All non secret configuration for n8n goes here...
      config:
        webhook_url: https://n8n.${DOMAIN}
        node:
          function_allow_builtin: "*"
        n8n:
          port: 5678
          proxy_hops: 1
          executions:
            pruneData: true
            pruneDataMaxAge: 336
          generic:
            timezone: ${TZ}
          db:
            type: postgresdb
            postgresdb:
              host: ai-automation-rw.ai-automation.svc.cozy.local
              port: 5432
              database: n8n
              user: ai-automation-cnpg
          queue:
            bull:
              redis:
                host: rfrm-n8n-redis.ai-automation.svc
                port: 6379
          community_packages:
            allow_tool_usage: true

      # All secret configuration for n8n goes here...
      secret:
        n8n:
          encryption_key:
            existingSecret: n8n-config
            existingSecretKey: encryption_key
          license:
            activation:
              key:
                existingSecret: n8n-config
                existingSecretKey: license_key
          db:
            postgresdb:
              password:
                existingSecret: ai-automation-db-creds
                existingSecretKey: password

      persistence:
        enabled: true
        type: existing
        existingClaim: n8n-data
        accessModes:
          - ReadWriteOnce
        size: 5Gi

      service:
        type: ClusterIP
        annotations:
          external-dns.alpha.kubernetes.io/internal: "true"
          external-dns.alpha.kubernetes.io/hostname: "n8n.${DOMAIN}"
          external-dns.alpha.kubernetes.io/target: "cluster-ingress.${DOMAIN}"
        port: 80
        targetPort: 5678

    ingress:
      enabled: true
      className: tenant-root
      annotations:
        external-dns.alpha.kubernetes.io/target: ${CLOUDFLARE_TUNNEL_TARGET}
        external-dns.alpha.kubernetes.io/cloudflare-proxied: "true"
        external-dns.alpha.kubernetes.io/hostname: "n8n.${DOMAIN}"
        # --- Authentik forward-auth (NGINX Ingress) ---
        nginx.ingress.kubernetes.io/auth-url: "http://authentik-server.authentik.svc.cozy.local/outpost.goauthentik.io/auth/nginx"
        nginx.ingress.kubernetes.io/auth-signin: "https://auth.${DOMAIN}/outpost.goauthentik.io/start?rd=$scheme://$http_host$escaped_request_uri"
        nginx.ingress.kubernetes.io/auth-response-headers: "Set-Cookie,X-authentik-username,X-authentik-email,X-authentik-name,X-authentik-uid,X-authentik-groups,X-authentik-jwt,X-authentik-meta"
        nginx.ingress.kubernetes.io/auth-snippet: |
          proxy_set_header X-Forwarded-Host $http_host;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header X-Forwarded-Uri $request_uri;
          proxy_set_header X-Original-Method $request_method;
        # --- Hopefully fixes n8n "Connection Lost" issues ---
        nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
        nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
        nginx.ingress.kubernetes.io/proxy-buffering: "off"
        nginx.ingress.kubernetes.io/proxy-http-version: "1.1"
        nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
      hosts:
        - host: n8n.${DOMAIN}
          paths:
            - path: /
              pathType: Prefix
      tls:
        - secretName: wildcard-le-tls-cert
          hosts:
            - n8n.${DOMAIN}

    resources:
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        memory: 2Gi

    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
      runAsGroup: 1000
      fsGroup: 1000
      fsGroupChangePolicy: OnRootMismatch

    podSecurityContext:
      runAsNonRoot: true
      runAsUser: 1000
      runAsGroup: 1000
      fsGroup: 1000

    livenessProbe:
      httpGet:
        path: /healthz
        port: 5678
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3

    readinessProbe:
      httpGet:
        path: /healthz
        port: 5678
      initialDelaySeconds: 10
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 3
